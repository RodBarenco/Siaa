# =============================================================
# SIAA ‚Äî Docker Compose
# Target: Oracle Cloud Free Tier ‚Äî ARM64 Ampere A1
# =============================================================
services:

  # -----------------------------------------------------------
  # 1. OLLAMA ‚Äî LLM local
  # -----------------------------------------------------------
  ollama:
    image: ollama/ollama:latest
    container_name: siaa-ollama
    restart: unless-stopped
    volumes:
      - ollama-data:/root/.ollama
    deploy:
      resources:
        limits:
          cpus: "3.0"
          memory: 12G
        reservations:
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # -----------------------------------------------------------
  # 2. OLLAMA-INIT ‚Äî Pull autom√°tico do modelo
  # -----------------------------------------------------------
  ollama-init:
    image: curlimages/curl:latest
    container_name: siaa-ollama-init
    depends_on:
      - ollama
    env_file:
      - .env
    entrypoint: ["sh", "-c"]
    command:
      - |
        echo "‚è≥ Aguardando Ollama..."
        sleep 10
        echo "üì¶ Baixando modelo: $${OLLAMA_MODEL_CHAT:-granite3.3:2b}..."
        curl -X POST http://ollama:11434/api/pull \
          -d "{\"name\": \"$${OLLAMA_MODEL_CHAT:-granite3.3:2b}\"}"
        echo "‚úÖ Modelo pronto!"

  # -----------------------------------------------------------
  # 3. SIAA ‚Äî Bot principal
  # -----------------------------------------------------------
  siaa:
    build:
      context: .
      dockerfile: src/siaa/Dockerfile
    container_name: siaa-bot
    restart: unless-stopped
    depends_on:
      - ollama
      - siaa-proxy
    env_file:
      - .env
    environment:
      # --- For√ßa o treinamento na inicializa√ß√£o ---
      FORCE_TRAIN: "true" 
      SIAA_DATA_DIR: /siaa-data
      OLLAMA_URL: http://siaa-ollama:11434
      PROXY_SERVER_URL: http://siaa-proxy:8001
      PYTHONUNBUFFERED: "1"
    volumes:
      - siaa-data:/siaa-data
      # C√≥digo √© lido direto da imagem (sem volumes para core)
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 10G
        reservations:
          memory: 512M
    healthcheck:
      test: ["CMD", "pgrep", "-f", "python.*app.py"]
      interval: 60s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  # -----------------------------------------------------------
  # 4. SIAA-PROXY ‚Äî Proxy para servi√ßos externos
  # -----------------------------------------------------------
  siaa-proxy:
    build:
      context: .
      dockerfile: src/siaa_proxy/Dockerfile
    container_name: siaa-proxy
    restart: unless-stopped
    ports:
      - "8001:8001"
    env_file:
      - .env
    volumes:
      - proxy-data:/proxy-data
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M

# =============================================================
# VOLUMES
# =============================================================
volumes:
  siaa-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/siaa-data

  ollama-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/ollama-data

  proxy-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/proxy-data

  vault-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/vault-data